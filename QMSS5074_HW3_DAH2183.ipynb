{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "QMSS5074_HW3_DAH2183.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "F1bIf3-U0lEF",
        "0oCysvh-07Ya",
        "HIJprtTr07jC",
        "plMCbXpEA-W5",
        "5EtBF0VT07t3",
        "LNHwGSC007yD",
        "rxK6hZi_1Mlj",
        "7CGSrn1S1SQI",
        "OOUuFW1O1VYc",
        "8ty9FMvd1_22"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F1bIf3-U0lEF",
        "colab_type": "text"
      },
      "source": [
        "## Daniel Howard, dah2183, QMSS 5074, Spring 2020 HW2\n",
        "## Assignment #3: Write up a report using BBC text classification data\n",
        "## Github Link : https://github.com/dhoward0497/QMSS5074_HW3\n",
        "#### Please note, all the code and plots are presented below. The write up of the report and any findings found are typed up after the code and at the end of this documet."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sxYHaA1F2J_m",
        "colab_type": "code",
        "outputId": "4bc302af-4af7-4a33-a432-2f7e0501d9fd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "## package imports\n",
        "\n",
        "import pandas as pd\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from nltk.stem import SnowballStemmer\n",
        "import nltk\n",
        "from string import punctuation\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords \n",
        "import re\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Flatten, Dense, Conv1D, Bidirectional, GRU, Embedding, MaxPooling1D, GlobalMaxPooling1D, LSTM\n",
        "from tensorflow.keras.optimizers import Adam, SGD\n",
        "from tensorflow.python.keras.callbacks import ReduceLROnPlateau\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.models import Sequential\n",
        "from sklearn.preprocessing import OneHotEncoder"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PTSJUVhx2Lj8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        },
        "outputId": "0167f861-99f5-47af-f45c-6d487880324d"
      },
      "source": [
        "## data import\n",
        "BBC_data = pd.read_csv(\"https://storage.googleapis.com/dataset-uploader/bbc/bbc-text.csv\")\n",
        "\n",
        "BBC_data.head()"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>category</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>tech</td>\n",
              "      <td>tv future in the hands of viewers with home th...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>business</td>\n",
              "      <td>worldcom boss  left books alone  former worldc...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>sport</td>\n",
              "      <td>tigers wary of farrell  gamble  leicester say ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>sport</td>\n",
              "      <td>yeading face newcastle in fa cup premiership s...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>entertainment</td>\n",
              "      <td>ocean s twelve raids box office ocean s twelve...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        category                                               text\n",
              "0           tech  tv future in the hands of viewers with home th...\n",
              "1       business  worldcom boss  left books alone  former worldc...\n",
              "2          sport  tigers wary of farrell  gamble  leicester say ...\n",
              "3          sport  yeading face newcastle in fa cup premiership s...\n",
              "4  entertainment  ocean s twelve raids box office ocean s twelve..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SbZ2PizShHR_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "cad09f92-ff79-46d9-87fd-c238b53c1cdd"
      },
      "source": [
        "print(\"Number of texts total :\", BBC_data[\"text\"].count())\n",
        "print(\"Number of unique text :\", len(list(set(i for i in BBC_data[\"text\"]))))\n",
        " ## some duplicate data wrt article names, need to remove duplicates\n",
        "\n",
        "BBC_data = BBC_data.drop_duplicates().reset_index(drop=True)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of texts total : 2225\n",
            "Number of unique text : 2126\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0oCysvh-07Ya",
        "colab_type": "text"
      },
      "source": [
        "## 1) Visualize the categories of your target variable and describe the dataset generally (the data includes news articles from the BBC news.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kgxwu2H62eXZ",
        "colab_type": "code",
        "outputId": "97ada552-1f42-4803-cd8d-9f4e59419397",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        }
      },
      "source": [
        "print(\"Number of categories :\", len(list(set(i for i in BBC_data[\"category\"]))))\n",
        "print(\"Category names       :\", [i for i in BBC_data[\"category\"].unique()])\n",
        "print(\"Category counts      :\")\n",
        "print(BBC_data[\"category\"].value_counts())"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of categories : 5\n",
            "Category names       : ['tech', 'business', 'sport', 'entertainment', 'politics']\n",
            "Category counts      :\n",
            "sport            504\n",
            "business         503\n",
            "politics         403\n",
            "entertainment    369\n",
            "tech             347\n",
            "Name: category, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OlCAbV5Dh5-l",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 341
        },
        "outputId": "f159cec0-139b-4710-ec33-a36e012c7ba0"
      },
      "source": [
        "BBC_data.groupby(\"category\").count().plot.bar()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bo3-g_plezPG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5ab7a6c6-d59d-4f08-eb78-5b1c96721891"
      },
      "source": [
        "total = 0\n",
        "count = 0\n",
        "for i in range(0, len(BBC_data[\"text\"])):\n",
        "  total+=int(len(BBC_data[\"text\"][i].split()))\n",
        "  count+=1\n",
        "avg_len = total/count\n",
        "print('The average text length is', avg_len)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The average text length is 390.45202257761053\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UVWGdVoe8eL_",
        "colab_type": "text"
      },
      "source": [
        "The  five target categories are tech, business, sport, entertainment, and politics. There are 347, 503, 504, 369 and 403 articles in each of the respective topics. Clearly, there were many more articles on sport and business than tech, politics and entertainement. The texts have on average 390 words.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HIJprtTr07jC",
        "colab_type": "text"
      },
      "source": [
        "## 2) Preprocess your data such that each document in the data is represented as a sequence of equal length"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PcjV78wKnZSH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "stemmer = SnowballStemmer(\"english\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hOzjgmxkGai_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "text_data = [i for i in BBC_data[\"text\"]]\n",
        "\n",
        "stop_words = set(stopwords.words('english') + list(punctuation))\n",
        "regexp_tokenizer = nltk.RegexpTokenizer(r\"\\w+\")\n",
        "\n",
        "original_length = []\n",
        "stemmed_length = []\n",
        "stemmed_text = []\n",
        "for row in text_data:\n",
        "  new_row = row.split(\" \")\n",
        "  original_length.append(len(new_row))\n",
        "  new_row = [x for x in new_row if len(x)>2]\n",
        "  new_row = \" \".join([str(elem) for elem in new_row if elem not in stop_words])\n",
        "  new_row = regexp_tokenizer.tokenize(new_row)\n",
        "  new_row = \" \".join([str(elem) for elem in new_row if elem not in stop_words])\n",
        "  new_row = re.sub(r'\\d+', '',new_row)\n",
        "\n",
        "  stemmed_length.append(len(new_row))\n",
        "  stemmed_text.append(new_row)\n",
        "\n",
        "BBC_data[\"text\"] = stemmed_text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c-t0s-p1kzyT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "texts = BBC_data[\"text\"]\n",
        "categories = BBC_data[\"category\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c4hvGBDtlEiG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "max_len = 250 # takes first 250 words only\n",
        "max_words = 10000\n",
        "train_len = 1500\n",
        "val_len = 500\n",
        "\n",
        "tokenizer = Tokenizer(num_words=max_words)\n",
        "tokenizer.fit_on_texts(texts)\n",
        "tokened_text = tokenizer.texts_to_sequences(texts)\n",
        "word_index = tokenizer.word_index\n",
        "\n",
        "text_data = pad_sequences(tokened_text, maxlen=max_len)\n",
        "\n",
        "indices = np.arange(text_data.shape[0])\n",
        "np.random.shuffle(indices)\n",
        "\n",
        "ohe = OneHotEncoder()\n",
        "y = ohe.fit_transform(np.asarray(categories).reshape(-1,1))\n",
        "\n",
        "text_data = text_data[indices]\n",
        "labels = y[indices]\n",
        "\n",
        "X_train = text_data[:train_len]\n",
        "y_train = labels[:train_len]\n",
        "\n",
        "X_val = text_data[train_len: train_len + val_len]\n",
        "y_val = labels[train_len: train_len + val_len]\n",
        "\n",
        "X_test = text_data[train_len + val_len:]\n",
        "y_test = labels[train_len + val_len:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "plMCbXpEA-W5",
        "colab_type": "text"
      },
      "source": [
        "##  3) Use the data to fit separate models to each of the following architectures:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5EtBF0VT07t3",
        "colab_type": "text"
      },
      "source": [
        "### A model with an embedding layer and dense layers (but w/ no layers meant for sequential data)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CSncHiF3pQrc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 818
        },
        "outputId": "139ea019-42a2-4488-fb40-6c8078dc3a27"
      },
      "source": [
        "model1 = Sequential()\n",
        "\n",
        "model1.add(Embedding(max_words, 256, input_length=max_len))\n",
        "\n",
        "model1.add(Flatten())\n",
        "\n",
        "model1.add(Dense(256, activation='sigmoid'))\n",
        "model1.add(Dense(32, activation = \"sigmoid\"))\n",
        "model1.add(Dense(5, activation='softmax'))\n",
        "\n",
        "model1.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "model1.summary()\n",
        "\n",
        "epochs = 20\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "model1.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=epochs, batch_size=batch_size)"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 1500 samples, validate on 500 samples\n",
            "Epoch 1/20\n",
            "1500/1500 [==============================] - 13s 9ms/step - loss: 1.4103 - accuracy: 0.3980 - val_loss: 1.0755 - val_accuracy: 0.6780\n",
            "Epoch 2/20\n",
            "1500/1500 [==============================] - 13s 9ms/step - loss: 0.5483 - accuracy: 0.9687 - val_loss: 0.4350 - val_accuracy: 0.9480\n",
            "Epoch 3/20\n",
            "1500/1500 [==============================] - 13s 9ms/step - loss: 0.2136 - accuracy: 1.0000 - val_loss: 0.2866 - val_accuracy: 0.9600\n",
            "Epoch 4/20\n",
            "1500/1500 [==============================] - 17s 12ms/step - loss: 0.1283 - accuracy: 1.0000 - val_loss: 0.2211 - val_accuracy: 0.9680\n",
            "Epoch 5/20\n",
            "1500/1500 [==============================] - 13s 9ms/step - loss: 0.0897 - accuracy: 1.0000 - val_loss: 0.1858 - val_accuracy: 0.9720\n",
            "Epoch 6/20\n",
            "1500/1500 [==============================] - 13s 9ms/step - loss: 0.0676 - accuracy: 1.0000 - val_loss: 0.1631 - val_accuracy: 0.9720\n",
            "Epoch 7/20\n",
            "1500/1500 [==============================] - 13s 9ms/step - loss: 0.0532 - accuracy: 1.0000 - val_loss: 0.1476 - val_accuracy: 0.9740\n",
            "Epoch 8/20\n",
            "1500/1500 [==============================] - 13s 9ms/step - loss: 0.0432 - accuracy: 1.0000 - val_loss: 0.1356 - val_accuracy: 0.9740\n",
            "Epoch 9/20\n",
            "1500/1500 [==============================] - 13s 9ms/step - loss: 0.0359 - accuracy: 1.0000 - val_loss: 0.1275 - val_accuracy: 0.9760\n",
            "Epoch 10/20\n",
            "1500/1500 [==============================] - 13s 8ms/step - loss: 0.0303 - accuracy: 1.0000 - val_loss: 0.1216 - val_accuracy: 0.9760\n",
            "Epoch 11/20\n",
            "1500/1500 [==============================] - 13s 8ms/step - loss: 0.0260 - accuracy: 1.0000 - val_loss: 0.1172 - val_accuracy: 0.9760\n",
            "Epoch 12/20\n",
            "1500/1500 [==============================] - 13s 8ms/step - loss: 0.0226 - accuracy: 1.0000 - val_loss: 0.1140 - val_accuracy: 0.9760\n",
            "Epoch 13/20\n",
            "1500/1500 [==============================] - 13s 8ms/step - loss: 0.0198 - accuracy: 1.0000 - val_loss: 0.1112 - val_accuracy: 0.9760\n",
            "Epoch 14/20\n",
            "1500/1500 [==============================] - 13s 8ms/step - loss: 0.0175 - accuracy: 1.0000 - val_loss: 0.1089 - val_accuracy: 0.9760\n",
            "Epoch 15/20\n",
            "1500/1500 [==============================] - 13s 8ms/step - loss: 0.0157 - accuracy: 1.0000 - val_loss: 0.1071 - val_accuracy: 0.9780\n",
            "Epoch 16/20\n",
            "1500/1500 [==============================] - 13s 8ms/step - loss: 0.0141 - accuracy: 1.0000 - val_loss: 0.1057 - val_accuracy: 0.9780\n",
            "Epoch 17/20\n",
            "1500/1500 [==============================] - 13s 8ms/step - loss: 0.0128 - accuracy: 1.0000 - val_loss: 0.1046 - val_accuracy: 0.9780\n",
            "Epoch 18/20\n",
            "1500/1500 [==============================] - 13s 8ms/step - loss: 0.0116 - accuracy: 1.0000 - val_loss: 0.1035 - val_accuracy: 0.9780\n",
            "Epoch 19/20\n",
            "1500/1500 [==============================] - 13s 8ms/step - loss: 0.0106 - accuracy: 1.0000 - val_loss: 0.1026 - val_accuracy: 0.9780\n",
            "Epoch 20/20\n",
            "1500/1500 [==============================] - 13s 8ms/step - loss: 0.0097 - accuracy: 1.0000 - val_loss: 0.1021 - val_accuracy: 0.9780\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.callbacks.History at 0x7eff7d2e8470>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LNHwGSC007yD",
        "colab_type": "text"
      },
      "source": [
        "### A model using an Embedding layer with Conv1d Layers\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pv2nVStQHlHc",
        "colab_type": "code",
        "outputId": "70479a04-ce40-472f-9331-5a3884b3cec3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model2 = Sequential()\n",
        "\n",
        "model2.add(Embedding(max_words, 256, input_length=max_len))\n",
        "\n",
        "model2.add(Conv1D(64, kernel_size=2, strides=1))\n",
        "model2.add(MaxPooling1D(2))\n",
        "\n",
        "model2.add(Conv1D(256, kernel_size=4, strides=2))\n",
        "model2.add(MaxPooling1D(2))\n",
        "\n",
        "model2.add(Flatten())\n",
        "\n",
        "model2.add(Dense(256, activation='relu'))\n",
        "model2.add(Dense(5, activation='softmax'))\n",
        "\n",
        "model2.compile(loss=\"categorical_crossentropy\",optimizer = \"adam\", metrics = [\"accuracy\"])\n",
        "model2.summary()\n",
        "\n",
        "history = model2.fit(X_train, y_train,\n",
        "                    epochs = 15, \n",
        "                    batch_size = 256,\n",
        "                    validation_data = (X_val,y_val))"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_3 (Embedding)      (None, 250, 256)          2560000   \n",
            "_________________________________________________________________\n",
            "conv1d_1 (Conv1D)            (None, 249, 64)           32832     \n",
            "_________________________________________________________________\n",
            "max_pooling1d_1 (MaxPooling1 (None, 124, 64)           0         \n",
            "_________________________________________________________________\n",
            "conv1d_2 (Conv1D)            (None, 61, 256)           65792     \n",
            "_________________________________________________________________\n",
            "max_pooling1d_2 (MaxPooling1 (None, 30, 256)           0         \n",
            "_________________________________________________________________\n",
            "flatten_3 (Flatten)          (None, 7680)              0         \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 256)               1966336   \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 5)                 1285      \n",
            "=================================================================\n",
            "Total params: 4,626,245\n",
            "Trainable params: 4,626,245\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 1500 samples, validate on 500 samples\n",
            "Epoch 1/15\n",
            "1500/1500 [==============================] - 4s 3ms/step - loss: 1.5270 - accuracy: 0.2967 - val_loss: 1.4762 - val_accuracy: 0.3540\n",
            "Epoch 2/15\n",
            "1500/1500 [==============================] - 4s 2ms/step - loss: 1.3466 - accuracy: 0.4400 - val_loss: 1.2818 - val_accuracy: 0.5140\n",
            "Epoch 3/15\n",
            "1500/1500 [==============================] - 4s 2ms/step - loss: 0.8671 - accuracy: 0.7600 - val_loss: 0.6520 - val_accuracy: 0.7720\n",
            "Epoch 4/15\n",
            "1500/1500 [==============================] - 4s 2ms/step - loss: 0.2366 - accuracy: 0.9473 - val_loss: 0.1872 - val_accuracy: 0.9460\n",
            "Epoch 5/15\n",
            "1500/1500 [==============================] - 4s 2ms/step - loss: 0.0258 - accuracy: 0.9933 - val_loss: 0.1342 - val_accuracy: 0.9660\n",
            "Epoch 6/15\n",
            "1500/1500 [==============================] - 4s 2ms/step - loss: 0.0047 - accuracy: 0.9987 - val_loss: 0.1593 - val_accuracy: 0.9600\n",
            "Epoch 7/15\n",
            "1500/1500 [==============================] - 4s 2ms/step - loss: 7.8363e-04 - accuracy: 1.0000 - val_loss: 0.1856 - val_accuracy: 0.9600\n",
            "Epoch 8/15\n",
            "1500/1500 [==============================] - 4s 2ms/step - loss: 7.1648e-05 - accuracy: 1.0000 - val_loss: 0.1901 - val_accuracy: 0.9620\n",
            "Epoch 9/15\n",
            "1500/1500 [==============================] - 4s 2ms/step - loss: 1.6102e-05 - accuracy: 1.0000 - val_loss: 0.1962 - val_accuracy: 0.9640\n",
            "Epoch 10/15\n",
            "1500/1500 [==============================] - 4s 2ms/step - loss: 8.3592e-06 - accuracy: 1.0000 - val_loss: 0.2012 - val_accuracy: 0.9620\n",
            "Epoch 11/15\n",
            "1500/1500 [==============================] - 4s 2ms/step - loss: 6.1294e-06 - accuracy: 1.0000 - val_loss: 0.2043 - val_accuracy: 0.9640\n",
            "Epoch 12/15\n",
            "1500/1500 [==============================] - 4s 2ms/step - loss: 4.8302e-06 - accuracy: 1.0000 - val_loss: 0.2060 - val_accuracy: 0.9640\n",
            "Epoch 13/15\n",
            "1500/1500 [==============================] - 4s 2ms/step - loss: 4.0749e-06 - accuracy: 1.0000 - val_loss: 0.2068 - val_accuracy: 0.9640\n",
            "Epoch 14/15\n",
            "1500/1500 [==============================] - 4s 2ms/step - loss: 3.7067e-06 - accuracy: 1.0000 - val_loss: 0.2070 - val_accuracy: 0.9640\n",
            "Epoch 15/15\n",
            "1500/1500 [==============================] - 4s 2ms/step - loss: 3.2899e-06 - accuracy: 1.0000 - val_loss: 0.2069 - val_accuracy: 0.9660\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rxK6hZi_1Mlj",
        "colab_type": "text"
      },
      "source": [
        "### A model using an Embedding layer with one sequential layer (LSTM or GRU)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ldOQ-i7Lsk7U",
        "colab_type": "code",
        "outputId": "393b42e7-c88c-4333-c5b7-8c1c70381ea0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 907
        }
      },
      "source": [
        "model3 = Sequential()\n",
        "\n",
        "model3.add(Embedding(max_words,256,input_length=max_len))\n",
        "\n",
        "model3.add(GRU(256))\n",
        "model3.add(Dense(256, activation='sigmoid'))\n",
        "model3.add(Dense(5, activation='softmax'))\n",
        "\n",
        "model3.compile(optimizer=\"adam\", loss='categorical_crossentropy', metrics=[\"accuracy\"])\n",
        "\n",
        "batch_size = 256\n",
        "\n",
        "model3.summary()\n",
        "\n",
        "history = model3.fit(X_train, y_train,\n",
        "                    epochs = 15, \n",
        "                    batch_size = 256,\n",
        "                    validation_data=(X_val, y_val))"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_5 (Embedding)      (None, 250, 256)          2560000   \n",
            "_________________________________________________________________\n",
            "gru_1 (GRU)                  (None, 256)               393984    \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 256)               65792     \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 5)                 1285      \n",
            "=================================================================\n",
            "Total params: 3,021,061\n",
            "Trainable params: 3,021,061\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 1500 samples, validate on 500 samples\n",
            "Epoch 1/15\n",
            "1500/1500 [==============================] - 28s 19ms/step - loss: 1.9726 - accuracy: 0.2307 - val_loss: 1.6668 - val_accuracy: 0.2620\n",
            "Epoch 2/15\n",
            "1500/1500 [==============================] - 27s 18ms/step - loss: 1.6312 - accuracy: 0.2360 - val_loss: 1.6282 - val_accuracy: 0.2200\n",
            "Epoch 3/15\n",
            "1500/1500 [==============================] - 27s 18ms/step - loss: 1.5597 - accuracy: 0.2447 - val_loss: 1.5843 - val_accuracy: 0.2240\n",
            "Epoch 4/15\n",
            "1500/1500 [==============================] - 27s 18ms/step - loss: 1.4129 - accuracy: 0.4273 - val_loss: 1.3062 - val_accuracy: 0.4520\n",
            "Epoch 5/15\n",
            "1500/1500 [==============================] - 27s 18ms/step - loss: 1.0797 - accuracy: 0.6393 - val_loss: 1.0935 - val_accuracy: 0.5420\n",
            "Epoch 6/15\n",
            "1500/1500 [==============================] - 28s 18ms/step - loss: 0.8667 - accuracy: 0.7160 - val_loss: 1.0830 - val_accuracy: 0.6060\n",
            "Epoch 7/15\n",
            "1500/1500 [==============================] - 27s 18ms/step - loss: 0.6206 - accuracy: 0.8160 - val_loss: 0.8517 - val_accuracy: 0.6800\n",
            "Epoch 8/15\n",
            "1500/1500 [==============================] - 27s 18ms/step - loss: 0.3952 - accuracy: 0.9027 - val_loss: 0.7618 - val_accuracy: 0.7260\n",
            "Epoch 9/15\n",
            "1500/1500 [==============================] - 27s 18ms/step - loss: 0.2536 - accuracy: 0.9787 - val_loss: 0.7385 - val_accuracy: 0.7300\n",
            "Epoch 10/15\n",
            "1500/1500 [==============================] - 28s 18ms/step - loss: 0.1351 - accuracy: 0.9880 - val_loss: 0.6897 - val_accuracy: 0.7700\n",
            "Epoch 11/15\n",
            "1500/1500 [==============================] - 27s 18ms/step - loss: 0.0764 - accuracy: 0.9953 - val_loss: 0.6221 - val_accuracy: 0.7920\n",
            "Epoch 12/15\n",
            "1500/1500 [==============================] - 27s 18ms/step - loss: 0.0369 - accuracy: 0.9980 - val_loss: 0.5804 - val_accuracy: 0.8540\n",
            "Epoch 13/15\n",
            "1500/1500 [==============================] - 27s 18ms/step - loss: 0.0200 - accuracy: 1.0000 - val_loss: 0.6609 - val_accuracy: 0.8160\n",
            "Epoch 14/15\n",
            "1500/1500 [==============================] - 28s 18ms/step - loss: 0.0124 - accuracy: 1.0000 - val_loss: 0.6472 - val_accuracy: 0.8480\n",
            "Epoch 15/15\n",
            "1500/1500 [==============================] - 27s 18ms/step - loss: 0.0089 - accuracy: 1.0000 - val_loss: 0.6141 - val_accuracy: 0.8560\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LxYVP1XM1Mn1",
        "colab_type": "text"
      },
      "source": [
        "### A model using an Embedding layer with stacked sequential layers (LSTM or GRU)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wQ8tmbvA6_O5",
        "colab_type": "code",
        "outputId": "5875e59b-38c4-4b23-a044-aded40e7d861",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 765
        }
      },
      "source": [
        "model4 = Sequential()\n",
        "\n",
        "model4.add(Embedding(max_words, 256, input_length=max_len))\n",
        "\n",
        "model4.add(GRU(256, return_sequences=True))\n",
        "model4.add(GRU(256))\n",
        "\n",
        "model4.add(Dense(256, activation='relu'))\n",
        "model4.add(Dense(5, activation='softmax'))\n",
        "\n",
        "model4.compile(optimizer=\"adam\", loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "model4.summary()\n",
        "\n",
        "history = model4.fit(X_train, y_train,\n",
        "                    epochs = 10, \n",
        "                    batch_size = 256,\n",
        "                    validation_data = (X_val, y_val))"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_9\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_8 (Embedding)      (None, 250, 256)          2560000   \n",
            "_________________________________________________________________\n",
            "gru_6 (GRU)                  (None, 250, 256)          393984    \n",
            "_________________________________________________________________\n",
            "gru_7 (GRU)                  (None, 256)               393984    \n",
            "_________________________________________________________________\n",
            "dense_13 (Dense)             (None, 256)               65792     \n",
            "_________________________________________________________________\n",
            "dense_14 (Dense)             (None, 5)                 1285      \n",
            "=================================================================\n",
            "Total params: 3,415,045\n",
            "Trainable params: 3,415,045\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 1500 samples, validate on 500 samples\n",
            "Epoch 1/10\n",
            "1500/1500 [==============================] - 60s 40ms/step - loss: 1.6011 - accuracy: 0.2567 - val_loss: 1.5806 - val_accuracy: 0.3500\n",
            "Epoch 2/10\n",
            "1500/1500 [==============================] - 55s 36ms/step - loss: 1.5086 - accuracy: 0.3873 - val_loss: 1.4078 - val_accuracy: 0.4360\n",
            "Epoch 3/10\n",
            "1500/1500 [==============================] - 55s 36ms/step - loss: 0.9842 - accuracy: 0.5767 - val_loss: 0.8332 - val_accuracy: 0.7060\n",
            "Epoch 4/10\n",
            "1500/1500 [==============================] - 55s 37ms/step - loss: 0.3806 - accuracy: 0.8833 - val_loss: 0.7098 - val_accuracy: 0.7620\n",
            "Epoch 5/10\n",
            "1500/1500 [==============================] - 54s 36ms/step - loss: 0.1178 - accuracy: 0.9660 - val_loss: 0.7628 - val_accuracy: 0.7980\n",
            "Epoch 6/10\n",
            "1500/1500 [==============================] - 55s 36ms/step - loss: 0.0441 - accuracy: 0.9907 - val_loss: 0.9514 - val_accuracy: 0.7740\n",
            "Epoch 7/10\n",
            "1500/1500 [==============================] - 55s 36ms/step - loss: 0.0061 - accuracy: 0.9980 - val_loss: 1.0572 - val_accuracy: 0.7780\n",
            "Epoch 8/10\n",
            "1500/1500 [==============================] - 55s 37ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 1.2041 - val_accuracy: 0.7560\n",
            "Epoch 9/10\n",
            "1500/1500 [==============================] - 55s 36ms/step - loss: 0.0051 - accuracy: 0.9987 - val_loss: 1.2006 - val_accuracy: 0.7680\n",
            "Epoch 10/10\n",
            "1500/1500 [==============================] - 55s 36ms/step - loss: 9.6907e-04 - accuracy: 1.0000 - val_loss: 1.2275 - val_accuracy: 0.7700\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7CGSrn1S1SQI",
        "colab_type": "text"
      },
      "source": [
        "### A model using an Embedding layer with bidirectional sequential layers\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MKSs9MCM_gbO",
        "colab_type": "code",
        "outputId": "4beb7446-9ce1-4291-f1cd-5ce4fd946491",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 907
        }
      },
      "source": [
        "model5 = Sequential()\n",
        "\n",
        "model5.add(Embedding(max_words, output_dim=32, input_length=max_len))\n",
        "\n",
        "model5.add(Bidirectional(GRU(256)))\n",
        "\n",
        "model5.add(Dense(256, activation='relu'))\n",
        "model5.add(Dense(5, activation='softmax'))\n",
        "\n",
        "model5.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=['accuracy'])\n",
        "\n",
        "\n",
        "model5.summary()\n",
        "\n",
        "\n",
        "history = model5.fit(X_train, y_train,\n",
        "                    epochs = 15, \n",
        "                    batch_size = 256,\n",
        "                    validation_data = (X_val, y_val))"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_13\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_11 (Embedding)     (None, 250, 32)           320000    \n",
            "_________________________________________________________________\n",
            "bidirectional_3 (Bidirection (None, 512)               443904    \n",
            "_________________________________________________________________\n",
            "dense_19 (Dense)             (None, 256)               131328    \n",
            "_________________________________________________________________\n",
            "dense_20 (Dense)             (None, 5)                 1285      \n",
            "=================================================================\n",
            "Total params: 896,517\n",
            "Trainable params: 896,517\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 1500 samples, validate on 500 samples\n",
            "Epoch 1/15\n",
            "1500/1500 [==============================] - 44s 29ms/step - loss: 0.4991 - accuracy: 0.8000 - val_loss: 0.4956 - val_accuracy: 0.8000\n",
            "Epoch 2/15\n",
            "1500/1500 [==============================] - 42s 28ms/step - loss: 0.4933 - accuracy: 0.8000 - val_loss: 0.4885 - val_accuracy: 0.8000\n",
            "Epoch 3/15\n",
            "1500/1500 [==============================] - 43s 28ms/step - loss: 0.4852 - accuracy: 0.8000 - val_loss: 0.4796 - val_accuracy: 0.8000\n",
            "Epoch 4/15\n",
            "1500/1500 [==============================] - 43s 28ms/step - loss: 0.4713 - accuracy: 0.8000 - val_loss: 0.4605 - val_accuracy: 0.8000\n",
            "Epoch 5/15\n",
            "1500/1500 [==============================] - 42s 28ms/step - loss: 0.5239 - accuracy: 0.7945 - val_loss: 0.4565 - val_accuracy: 0.8000\n",
            "Epoch 6/15\n",
            "1500/1500 [==============================] - 43s 28ms/step - loss: 0.4280 - accuracy: 0.8012 - val_loss: 0.4552 - val_accuracy: 0.8032\n",
            "Epoch 7/15\n",
            "1500/1500 [==============================] - 43s 28ms/step - loss: 0.3994 - accuracy: 0.8176 - val_loss: 0.4284 - val_accuracy: 0.8160\n",
            "Epoch 8/15\n",
            "1500/1500 [==============================] - 43s 28ms/step - loss: 0.3523 - accuracy: 0.8415 - val_loss: 0.3918 - val_accuracy: 0.8304\n",
            "Epoch 9/15\n",
            "1500/1500 [==============================] - 47s 31ms/step - loss: 0.2862 - accuracy: 0.8980 - val_loss: 0.3607 - val_accuracy: 0.8520\n",
            "Epoch 10/15\n",
            "1500/1500 [==============================] - 43s 29ms/step - loss: 0.2418 - accuracy: 0.9108 - val_loss: 0.3972 - val_accuracy: 0.8320\n",
            "Epoch 11/15\n",
            "1500/1500 [==============================] - 43s 28ms/step - loss: 0.1855 - accuracy: 0.9351 - val_loss: 0.3349 - val_accuracy: 0.8616\n",
            "Epoch 12/15\n",
            "1500/1500 [==============================] - 43s 29ms/step - loss: 0.1349 - accuracy: 0.9573 - val_loss: 0.3168 - val_accuracy: 0.8744\n",
            "Epoch 13/15\n",
            "1500/1500 [==============================] - 43s 28ms/step - loss: 0.0850 - accuracy: 0.9709 - val_loss: 0.2914 - val_accuracy: 0.8800\n",
            "Epoch 14/15\n",
            "1500/1500 [==============================] - 43s 29ms/step - loss: 0.0490 - accuracy: 0.9884 - val_loss: 0.3076 - val_accuracy: 0.8792\n",
            "Epoch 15/15\n",
            "1500/1500 [==============================] - 42s 28ms/step - loss: 0.0286 - accuracy: 0.9944 - val_loss: 0.3060 - val_accuracy: 0.8828\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OOUuFW1O1VYc",
        "colab_type": "text"
      },
      "source": [
        "### Now retrain your best model from C, D, and E using dropout (you may need to increase epochs!)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pX0KfNPy6kcr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b047ce28-1214-4764-c15a-39b978a93eed"
      },
      "source": [
        "model6 = Sequential()\n",
        "\n",
        "model6.add(Embedding(max_words,256,input_length=max_len))\n",
        "\n",
        "model6.add(GRU(256,dropout=.5))\n",
        "model6.add(Dense(256, activation='sigmoid'))\n",
        "model6.add(Dense(5, activation='softmax'))\n",
        "\n",
        "model6.compile(optimizer=\"adam\", loss='categorical_crossentropy', metrics=[\"accuracy\"])\n",
        "\n",
        "batch_size = 256\n",
        "\n",
        "model6.summary()\n",
        "\n",
        "history = model6.fit(X_train, y_train,\n",
        "                    epochs = 30, \n",
        "                    batch_size = 256,\n",
        "                    validation_data=(X_val, y_val))"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_15\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_13 (Embedding)     (None, 250, 256)          2560000   \n",
            "_________________________________________________________________\n",
            "gru_12 (GRU)                 (None, 256)               393984    \n",
            "_________________________________________________________________\n",
            "dense_23 (Dense)             (None, 256)               65792     \n",
            "_________________________________________________________________\n",
            "dense_24 (Dense)             (None, 5)                 1285      \n",
            "=================================================================\n",
            "Total params: 3,021,061\n",
            "Trainable params: 3,021,061\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 1500 samples, validate on 500 samples\n",
            "Epoch 1/30\n",
            "1500/1500 [==============================] - 28s 19ms/step - loss: 1.7335 - accuracy: 0.2053 - val_loss: 1.6449 - val_accuracy: 0.2200\n",
            "Epoch 2/30\n",
            "1500/1500 [==============================] - 28s 18ms/step - loss: 1.6028 - accuracy: 0.2460 - val_loss: 1.5922 - val_accuracy: 0.2620\n",
            "Epoch 3/30\n",
            "1500/1500 [==============================] - 27s 18ms/step - loss: 1.5812 - accuracy: 0.2307 - val_loss: 1.5556 - val_accuracy: 0.2620\n",
            "Epoch 4/30\n",
            "1500/1500 [==============================] - 27s 18ms/step - loss: 1.4637 - accuracy: 0.4293 - val_loss: 1.3792 - val_accuracy: 0.6020\n",
            "Epoch 5/30\n",
            "1500/1500 [==============================] - 27s 18ms/step - loss: 1.5250 - accuracy: 0.5640 - val_loss: 1.3535 - val_accuracy: 0.4020\n",
            "Epoch 6/30\n",
            "1500/1500 [==============================] - 28s 18ms/step - loss: 1.1274 - accuracy: 0.6380 - val_loss: 1.3106 - val_accuracy: 0.4780\n",
            "Epoch 7/30\n",
            "1500/1500 [==============================] - 27s 18ms/step - loss: 0.8885 - accuracy: 0.7713 - val_loss: 1.0283 - val_accuracy: 0.4780\n",
            "Epoch 8/30\n",
            "1500/1500 [==============================] - 27s 18ms/step - loss: 0.6434 - accuracy: 0.8400 - val_loss: 0.8703 - val_accuracy: 0.6700\n",
            "Epoch 9/30\n",
            "1500/1500 [==============================] - 27s 18ms/step - loss: 0.4759 - accuracy: 0.9367 - val_loss: 0.7430 - val_accuracy: 0.7580\n",
            "Epoch 10/30\n",
            "1500/1500 [==============================] - 27s 18ms/step - loss: 0.2947 - accuracy: 0.9753 - val_loss: 0.6133 - val_accuracy: 0.7980\n",
            "Epoch 11/30\n",
            "1500/1500 [==============================] - 33s 22ms/step - loss: 0.1445 - accuracy: 0.9880 - val_loss: 0.5024 - val_accuracy: 0.8400\n",
            "Epoch 12/30\n",
            "1500/1500 [==============================] - 27s 18ms/step - loss: 0.1149 - accuracy: 0.9787 - val_loss: 0.5639 - val_accuracy: 0.8100\n",
            "Epoch 13/30\n",
            "1500/1500 [==============================] - 27s 18ms/step - loss: 0.0797 - accuracy: 0.9900 - val_loss: 0.5403 - val_accuracy: 0.8080\n",
            "Epoch 14/30\n",
            "1500/1500 [==============================] - 27s 18ms/step - loss: 0.0543 - accuracy: 0.9967 - val_loss: 0.5237 - val_accuracy: 0.8480\n",
            "Epoch 15/30\n",
            "1500/1500 [==============================] - 27s 18ms/step - loss: 0.0306 - accuracy: 1.0000 - val_loss: 0.5350 - val_accuracy: 0.8440\n",
            "Epoch 16/30\n",
            "1500/1500 [==============================] - 27s 18ms/step - loss: 0.0226 - accuracy: 1.0000 - val_loss: 0.4881 - val_accuracy: 0.8440\n",
            "Epoch 17/30\n",
            "1500/1500 [==============================] - 27s 18ms/step - loss: 0.0138 - accuracy: 1.0000 - val_loss: 0.4510 - val_accuracy: 0.8540\n",
            "Epoch 18/30\n",
            "1500/1500 [==============================] - 27s 18ms/step - loss: 0.0112 - accuracy: 0.9993 - val_loss: 0.4372 - val_accuracy: 0.8660\n",
            "Epoch 19/30\n",
            "1500/1500 [==============================] - 27s 18ms/step - loss: 0.0085 - accuracy: 1.0000 - val_loss: 0.4286 - val_accuracy: 0.8760\n",
            "Epoch 20/30\n",
            "1500/1500 [==============================] - 27s 18ms/step - loss: 0.0075 - accuracy: 1.0000 - val_loss: 0.4409 - val_accuracy: 0.8720\n",
            "Epoch 21/30\n",
            "1500/1500 [==============================] - 27s 18ms/step - loss: 0.0068 - accuracy: 1.0000 - val_loss: 0.4246 - val_accuracy: 0.8740\n",
            "Epoch 22/30\n",
            "1500/1500 [==============================] - 27s 18ms/step - loss: 0.0053 - accuracy: 1.0000 - val_loss: 0.4096 - val_accuracy: 0.8780\n",
            "Epoch 23/30\n",
            "1500/1500 [==============================] - 27s 18ms/step - loss: 0.0045 - accuracy: 1.0000 - val_loss: 0.4070 - val_accuracy: 0.8860\n",
            "Epoch 24/30\n",
            "1500/1500 [==============================] - 27s 18ms/step - loss: 0.0042 - accuracy: 1.0000 - val_loss: 0.4077 - val_accuracy: 0.8840\n",
            "Epoch 25/30\n",
            "1500/1500 [==============================] - 27s 18ms/step - loss: 0.0094 - accuracy: 0.9993 - val_loss: 0.4604 - val_accuracy: 0.8620\n",
            "Epoch 26/30\n",
            "1500/1500 [==============================] - 27s 18ms/step - loss: 0.0069 - accuracy: 0.9993 - val_loss: 0.4391 - val_accuracy: 0.8740\n",
            "Epoch 27/30\n",
            "1500/1500 [==============================] - 27s 18ms/step - loss: 0.0057 - accuracy: 1.0000 - val_loss: 0.4261 - val_accuracy: 0.8740\n",
            "Epoch 28/30\n",
            "1500/1500 [==============================] - 27s 18ms/step - loss: 0.0038 - accuracy: 1.0000 - val_loss: 0.4053 - val_accuracy: 0.8800\n",
            "Epoch 29/30\n",
            "1500/1500 [==============================] - 27s 18ms/step - loss: 0.0040 - accuracy: 1.0000 - val_loss: 0.4058 - val_accuracy: 0.8820\n",
            "Epoch 30/30\n",
            "1500/1500 [==============================] - 27s 18ms/step - loss: 0.0038 - accuracy: 1.0000 - val_loss: 0.4077 - val_accuracy: 0.8840\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ty9FMvd1_22",
        "colab_type": "text"
      },
      "source": [
        "## Discuss which model(s) performed best and speculate about how you might try to further improve the predictive power of your model (e.g. Glove embeddings? More layers? Combining Conv1D with LSTM layers? More LSTM hidden nodes?)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FBV9cMaj7bak",
        "colab_type": "text"
      },
      "source": [
        "The best model was the model with Conv1D layers, which had a val_accuracy score of 0.9660. This is surprising as it is expected that the model with bidirectional layers would be the one to best perform, as it can process and \"read\" text data in both directions. This helps it understand the text and the meaning of the words within the context of the text.\n",
        "\n",
        "Using glove embeddings and more layers could have improved the model. It would also be interesting to see if adding more Conv1d layers or combining with bidrectional GRU layers would improve the model score. However, at 0.966, it is already very accurate."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tdr0CY6i2FLX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}